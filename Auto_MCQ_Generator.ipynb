{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6/R2brfnQK2oEYqaG5WF2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dheerajtripathi21/node.js/blob/main/Auto_MCQ_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "import re\n",
        "import PyPDF2\n",
        "import spacy\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# ==============================\n",
        "# Load SpaCy model\n",
        "# ==============================\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ==============================\n",
        "# Load Question Generation Model\n",
        "# ==============================\n",
        "QG_MODEL_NAME = \"valhalla/t5-small-qg-hl\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(QG_MODEL_NAME)\n",
        "qg_model = AutoModelForSeq2SeqLM.from_pretrained(QG_MODEL_NAME)\n",
        "qg_pipeline = pipeline(\"text2text-generation\", model=qg_model, tokenizer=tokenizer)\n",
        "\n",
        "# ==============================\n",
        "# Utility: Read PDF\n",
        "# ==============================\n",
        "def read_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"Extracts text from a PDF file path.\"\"\"\n",
        "    if not pdf_path:\n",
        "        return \"\"\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            try:\n",
        "                txt = page.extract_text()\n",
        "            except Exception:\n",
        "                txt = \"\"\n",
        "            if txt:\n",
        "                text += txt + \" \"\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ==============================\n",
        "# Extract keywords\n",
        "# ==============================\n",
        "def extract_keywords(text: str, top_n: int = 5):\n",
        "    \"\"\"Pick key phrases from text (noun chunks).\"\"\"\n",
        "    doc = nlp(text)\n",
        "    chunks = []\n",
        "    for chunk in doc.noun_chunks:\n",
        "        phrase = chunk.text.strip()\n",
        "        if len(phrase.split()) > 1:\n",
        "            chunks.append(phrase)\n",
        "    seen = set()\n",
        "    keywords = []\n",
        "    for k in chunks:\n",
        "        key = k.lower()\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            keywords.append(k)\n",
        "        if len(keywords) >= top_n:\n",
        "            break\n",
        "    if len(keywords) < top_n:\n",
        "        singles = [t.text for t in doc if t.pos_ in {\"PROPN\", \"NOUN\"} and t.is_alpha]\n",
        "        for s in singles:\n",
        "            if s.lower() not in seen:\n",
        "                seen.add(s.lower())\n",
        "                keywords.append(s)\n",
        "            if len(keywords) >= top_n:\n",
        "                break\n",
        "    return keywords[:top_n]\n",
        "\n",
        "# ==============================\n",
        "# Build context for QG\n",
        "# ==============================\n",
        "def safe_highlight_context(text: str, answer: str, max_len: int = 600):\n",
        "    \"\"\"Add <hl> tags around answer and trim context.\"\"\"\n",
        "    pattern = re.compile(re.escape(answer), re.IGNORECASE)\n",
        "    match = pattern.search(text)\n",
        "    if match:\n",
        "        start, end = match.start(), match.end()\n",
        "        context = text[:start] + \"<hl> \" + text[start:end] + \" <hl>\" + text[end:]\n",
        "    else:\n",
        "        context = text + f\" <hl> {answer} <hl>\"\n",
        "    hl_pos = context.lower().find(\"<hl>\")\n",
        "    if hl_pos != -1:\n",
        "        left = max(0, hl_pos - max_len // 2)\n",
        "        right = min(len(context), hl_pos + max_len // 2)\n",
        "        context = context[left:right]\n",
        "    else:\n",
        "        context = context[:max_len]\n",
        "    return \"generate question: \" + context\n",
        "\n",
        "# ==============================\n",
        "# Generate MCQs\n",
        "# ==============================\n",
        "def generate_mcqs(text: str, num_questions: int = 5, num_options: int = 4):\n",
        "    text = (text or \"\").strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    text = text[:5000]\n",
        "    answers = extract_keywords(text, top_n=num_questions)\n",
        "    if not answers:\n",
        "        return []\n",
        "    fallback_pool = [\"Technology\", \"Science\", \"Education\", \"History\", \"Geography\", \"Mathematics\", \"Biology\"]\n",
        "    mcqs = []\n",
        "    for ans in answers:\n",
        "        try:\n",
        "            prompt = safe_highlight_context(text, ans)\n",
        "            out = qg_pipeline(prompt, max_length=72, do_sample=False, num_return_sequences=1)\n",
        "            question = out[0][\"generated_text\"].strip()\n",
        "            if not question.endswith(\"?\"):\n",
        "                question += \"?\"\n",
        "        except Exception:\n",
        "            question = f\"What is '{ans}'?\"\n",
        "        distractors = [a for a in answers if a != ans]\n",
        "        while len(distractors) < (num_options - 1):\n",
        "            pick = random.choice(fallback_pool)\n",
        "            if pick not in distractors and pick.lower() != ans.lower():\n",
        "                distractors.append(pick)\n",
        "        options = distractors[: (num_options - 1)] + [ans]\n",
        "        random.shuffle(options)\n",
        "        mcqs.append({\n",
        "            \"question\": question,\n",
        "            \"options\": options,\n",
        "            \"answer\": ans\n",
        "        })\n",
        "    return mcqs\n",
        "\n",
        "# ==============================\n",
        "# Gradio UI\n",
        "# ==============================\n",
        "MAX_Q = 10\n",
        "with gr.Blocks(fill_height=True) as demo:\n",
        "    gr.Markdown(\"## üìù Automatic MCQ Generator (Paste Text OR Upload PDF)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        text_input = gr.Textbox(\n",
        "            label=\"‚úçÔ∏è Paste your text here\",\n",
        "            lines=8,\n",
        "            placeholder=\"Paste text here... (or upload a PDF on the right)\"\n",
        "        )\n",
        "        file_input = gr.File(\n",
        "            label=\"üìÇ Or upload a PDF\",\n",
        "            type=\"filepath\"\n",
        "        )\n",
        "\n",
        "    num_q = gr.Slider(1, MAX_Q, step=1, value=5, label=\"How many questions?\")\n",
        "    generate_btn = gr.Button(\"üöÄ Generate Quiz\", variant=\"primary\")\n",
        "\n",
        "    status_md = gr.Markdown()\n",
        "    quiz_data_state = gr.State([])\n",
        "\n",
        "    radios, results = [], []\n",
        "    with gr.Column():\n",
        "        for i in range(MAX_Q):\n",
        "            q_radio = gr.Radio(choices=[], label=f\"Q{i+1}.\", interactive=True, visible=False)\n",
        "            q_result = gr.Markdown(visible=False)\n",
        "            radios.append(q_radio)\n",
        "            results.append(q_result)\n",
        "\n",
        "    submit_btn = gr.Button(\"‚úÖ Submit All\")\n",
        "    score_md = gr.Markdown()\n",
        "\n",
        "    # ==============================\n",
        "    # Handlers\n",
        "    # ==============================\n",
        "    def on_generate(input_text, pdf_path, n_questions):\n",
        "        if pdf_path:\n",
        "            text = read_pdf(pdf_path)\n",
        "        else:\n",
        "            text = input_text or \"\"\n",
        "        mcqs = generate_mcqs(text, num_questions=int(n_questions))\n",
        "        if not mcqs:\n",
        "            updates = [gr.update(value=\"‚ùå Please paste text or upload a readable PDF.\", visible=True)]\n",
        "            for _ in range(MAX_Q):\n",
        "                updates += [gr.update(choices=[], label=\"\", value=None, visible=False),\n",
        "                            gr.update(value=\"\", visible=False)]\n",
        "            updates.append([])\n",
        "            return updates\n",
        "        updates = [gr.update(value=f\"‚úÖ Generated {len(mcqs)} questions. Select answers below ‚¨áÔ∏è\", visible=True)]\n",
        "        for i in range(MAX_Q):\n",
        "            if i < len(mcqs):\n",
        "                q = mcqs[i]\n",
        "                updates += [\n",
        "                    gr.update(choices=q[\"options\"], label=f\"Q{i+1}. {q['question']}\", value=None, visible=True),\n",
        "                    gr.update(value=\"\", visible=True)\n",
        "                ]\n",
        "            else:\n",
        "                updates += [gr.update(choices=[], label=f\"Q{i+1}.\", value=None, visible=False),\n",
        "                            gr.update(value=\"\", visible=False)]\n",
        "        updates.append(mcqs)\n",
        "        return updates\n",
        "\n",
        "    generate_btn.click(\n",
        "        on_generate,\n",
        "        inputs=[text_input, file_input, num_q],\n",
        "        outputs=[status_md] + sum(([radios[i], results[i]] for i in range(MAX_Q)), []) + [quiz_data_state]\n",
        "    )\n",
        "\n",
        "    def check_single(choice, quiz_data, idx):\n",
        "        if not quiz_data or idx >= len(quiz_data) or choice is None:\n",
        "            return \"\"\n",
        "        correct = quiz_data[idx][\"answer\"]\n",
        "        return \"‚úÖ Correct!\" if choice == correct else f\"‚ùå Wrong! **Correct:** {correct}\"\n",
        "\n",
        "    for i in range(MAX_Q):\n",
        "        radios[i].change(\n",
        "            lambda choice, data, idx=i: check_single(choice, data, idx),\n",
        "            inputs=[radios[i], quiz_data_state],\n",
        "            outputs=results[i]\n",
        "        )\n",
        "\n",
        "    def score_all(*args):\n",
        "        *answers, quiz_data = args\n",
        "        if not quiz_data:\n",
        "            return \"‚ÑπÔ∏è Generate a quiz first.\"\n",
        "        total = len(quiz_data)\n",
        "        correct = 0\n",
        "        lines = []\n",
        "        for i in range(total):\n",
        "            sel = answers[i] if i < len(answers) else None\n",
        "            ans = quiz_data[i][\"answer\"]\n",
        "            if sel == ans:\n",
        "                correct += 1\n",
        "                lines.append(f\"**Q{i+1}.** ‚úÖ Correct\")\n",
        "            elif sel is None:\n",
        "                lines.append(f\"**Q{i+1}.** ‚è≠Ô∏è Skipped (Correct: **{ans}**)\")\n",
        "            else:\n",
        "                lines.append(f\"**Q{i+1}.** ‚ùå Wrong (Your: _{sel}_ | Correct: **{ans}**)\")\n",
        "        lines.append(f\"\\n### üéØ Final Score: **{correct}/{total}**\")\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    submit_btn.click(\n",
        "        score_all,\n",
        "        inputs=radios + [quiz_data_state],\n",
        "        outputs=score_md\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3Fed2YlC3i6",
        "outputId": "879bb243-5806-486b-a550-95a5f75bfe8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lO0QGzxPIifs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy transformers sentencepiece PyPDF2 gradio\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLbWpgkXIEnk",
        "outputId": "69af5dad-950f-46b9-82d8-ed5e4c850506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.42.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.11.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.11.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.11.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Markdown(\"\"\"\n",
        "# üåü AI-Powered MCQ Generator\n",
        "Upload a PDF or Paste Text ‚Üí Get **Instant Quiz Questions**!\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "BR-U3H_KJfdP",
        "outputId": "0e45a9a5-77af-4d69-968a-164b87285cba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gradio.components.markdown.Markdown at 0x7b45a175f710>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re\n",
        "import PyPDF2\n",
        "import spacy\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# ==============================\n",
        "# Load SpaCy model\n",
        "# ==============================\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ==============================\n",
        "# Load Question Generation Model\n",
        "# ==============================\n",
        "QG_MODEL_NAME = \"valhalla/t5-small-qg-hl\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(QG_MODEL_NAME)\n",
        "qg_model = AutoModelForSeq2SeqLM.from_pretrained(QG_MODEL_NAME)\n",
        "qg_pipeline = pipeline(\"text2text-generation\", model=qg_model, tokenizer=tokenizer)\n",
        "\n",
        "# ==============================\n",
        "# Utility: Read PDF\n",
        "# ==============================\n",
        "def read_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"Extracts text from a PDF file path.\"\"\"\n",
        "    if not pdf_path:\n",
        "        return \"\"\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            try:\n",
        "                txt = page.extract_text()\n",
        "            except Exception:\n",
        "                txt = \"\"\n",
        "            if txt:\n",
        "                text += txt + \" \"\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ==============================\n",
        "# Extract keywords\n",
        "# ==============================\n",
        "def extract_keywords(text: str, top_n: int = 5):\n",
        "    \"\"\"Pick key phrases from text (noun chunks).\"\"\"\n",
        "    doc = nlp(text)\n",
        "    chunks = []\n",
        "    for chunk in doc.noun_chunks:\n",
        "        phrase = chunk.text.strip()\n",
        "        if len(phrase.split()) > 1:\n",
        "            chunks.append(phrase)\n",
        "    seen = set()\n",
        "    keywords = []\n",
        "    for k in chunks:\n",
        "        key = k.lower()\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            keywords.append(k)\n",
        "        if len(keywords) >= top_n:\n",
        "            break\n",
        "    if len(keywords) < top_n:\n",
        "        singles = [t.text for t in doc if t.pos_ in {\"PROPN\", \"NOUN\"} and t.is_alpha]\n",
        "        for s in singles:\n",
        "            if s.lower() not in seen:\n",
        "                seen.add(s.lower())\n",
        "                keywords.append(s)\n",
        "            if len(keywords) >= top_n:\n",
        "                break\n",
        "    return keywords[:top_n]\n",
        "\n",
        "# ==============================\n",
        "# Build context for QG\n",
        "# ==============================\n",
        "def safe_highlight_context(text: str, answer: str, max_len: int = 600):\n",
        "    \"\"\"Add <hl> tags around answer and trim context.\"\"\"\n",
        "    pattern = re.compile(re.escape(answer), re.IGNORECASE)\n",
        "    match = pattern.search(text)\n",
        "    if match:\n",
        "        start, end = match.start(), match.end()\n",
        "        context = text[:start] + \"<hl> \" + text[start:end] + \" <hl>\" + text[end:]\n",
        "    else:\n",
        "        context = text + f\" <hl> {answer} <hl>\"\n",
        "    hl_pos = context.lower().find(\"<hl>\")\n",
        "    if hl_pos != -1:\n",
        "        left = max(0, hl_pos - max_len // 2)\n",
        "        right = min(len(context), hl_pos + max_len // 2)\n",
        "        context = context[left:right]\n",
        "    else:\n",
        "        context = context[:max_len]\n",
        "    return \"generate question: \" + context\n",
        "\n",
        "# ==============================\n",
        "# Generate MCQs\n",
        "# ==============================\n",
        "def generate_mcqs(text: str, num_questions: int = 5, num_options: int = 4):\n",
        "    text = (text or \"\").strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    text = text[:5000]\n",
        "    answers = extract_keywords(text, top_n=num_questions)\n",
        "    if not answers:\n",
        "        return []\n",
        "    fallback_pool = [\"Technology\", \"Science\", \"Education\", \"History\", \"Geography\", \"Mathematics\", \"Biology\"]\n",
        "    mcqs = []\n",
        "    for ans in answers:\n",
        "        try:\n",
        "            prompt = safe_highlight_context(text, ans)\n",
        "            out = qg_pipeline(prompt, max_length=72, do_sample=False, num_return_sequences=1)\n",
        "            question = out[0][\"generated_text\"].strip()\n",
        "            if not question.endswith(\"?\"):\n",
        "                question += \"?\"\n",
        "        except Exception:\n",
        "            question = f\"What is '{ans}'?\"\n",
        "        distractors = [a for a in answers if a != ans]\n",
        "        while len(distractors) < (num_options - 1):\n",
        "            pick = random.choice(fallback_pool)\n",
        "            if pick not in distractors and pick.lower() != ans.lower():\n",
        "                distractors.append(pick)\n",
        "        options = distractors[: (num_options - 1)] + [ans]\n",
        "        random.shuffle(options)\n",
        "        mcqs.append({\n",
        "            \"question\": question,\n",
        "            \"options\": options,\n",
        "            \"answer\": ans\n",
        "        })\n",
        "    return mcqs\n",
        "\n",
        "# ==============================\n",
        "# Gradio UI\n",
        "# ==============================\n",
        "MAX_Q = 10\n",
        "with gr.Blocks(fill_height=True) as demo:\n",
        "    gr.Markdown(\"## üìù Automatic MCQ Generator (Paste Text OR Upload PDF)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        text_input = gr.Textbox(\n",
        "            label=\"‚úçÔ∏è Paste your text here\",\n",
        "            lines=8,\n",
        "            placeholder=\"Paste text here... (or upload a PDF on the right)\"\n",
        "        )\n",
        "        file_input = gr.File(\n",
        "            label=\"üìÇ Or upload a PDF\",\n",
        "            type=\"filepath\"\n",
        "        )\n",
        "\n",
        "    num_q = gr.Slider(1, MAX_Q, step=1, value=5, label=\"How many questions?\")\n",
        "    generate_btn = gr.Button(\"üöÄ Generate Quiz\", variant=\"primary\")\n",
        "\n",
        "    status_md = gr.Markdown()\n",
        "    quiz_data_state = gr.State([])\n",
        "\n",
        "    radios, results = [], []\n",
        "    with gr.Column():\n",
        "        for i in range(MAX_Q):\n",
        "            q_radio = gr.Radio(choices=[], label=f\"Q{i+1}.\", interactive=True, visible=False)\n",
        "            q_result = gr.Markdown(visible=False)\n",
        "            radios.append(q_radio)\n",
        "            results.append(q_result)\n",
        "\n",
        "    submit_btn = gr.Button(\"‚úÖ Submit All\")\n",
        "    score_md = gr.Markdown()\n",
        "\n",
        "    # ==============================\n",
        "    # Handlers\n",
        "    # ==============================\n",
        "    def on_generate(input_text, pdf_path, n_questions):\n",
        "        if pdf_path:\n",
        "            text = read_pdf(pdf_path)\n",
        "        else:\n",
        "            text = input_text or \"\"\n",
        "        mcqs = generate_mcqs(text, num_questions=int(n_questions))\n",
        "        if not mcqs:\n",
        "            updates = [gr.update(value=\"‚ùå Please paste text or upload a readable PDF.\", visible=True)]\n",
        "            for _ in range(MAX_Q):\n",
        "                updates += [gr.update(choices=[], label=\"\", value=None, visible=False),\n",
        "                            gr.update(value=\"\", visible=False)]\n",
        "            updates.append([])\n",
        "            return updates\n",
        "        updates = [gr.update(value=f\"‚úÖ Generated {len(mcqs)} questions. Select answers below ‚¨áÔ∏è\", visible=True)]\n",
        "        for i in range(MAX_Q):\n",
        "            if i < len(mcqs):\n",
        "                q = mcqs[i]\n",
        "                updates += [\n",
        "                    gr.update(choices=q[\"options\"], label=f\"Q{i+1}. {q['question']}\", value=None, visible=True),\n",
        "                    gr.update(value=\"\", visible=True)\n",
        "                ]\n",
        "            else:\n",
        "                updates += [gr.update(choices=[], label=f\"Q{i+1}.\", value=None, visible=False),\n",
        "                            gr.update(value=\"\", visible=False)]\n",
        "        updates.append(mcqs)\n",
        "        return updates\n",
        "\n",
        "    generate_btn.click(\n",
        "        on_generate,\n",
        "        inputs=[text_input, file_input, num_q],\n",
        "        outputs=[status_md] + sum(([radios[i], results[i]] for i in range(MAX_Q)), []) + [quiz_data_state]\n",
        "    )\n",
        "\n",
        "    def check_single(choice, quiz_data, idx):\n",
        "        if not quiz_data or idx >= len(quiz_data) or choice is None:\n",
        "            return \"\"\n",
        "        correct = quiz_data[idx][\"answer\"]\n",
        "        return \"‚úÖ Correct!\" if choice == correct else f\"‚ùå Wrong! **Correct:** {correct}\"\n",
        "\n",
        "    for i in range(MAX_Q):\n",
        "        radios[i].change(\n",
        "            lambda choice, data, idx=i: check_single(choice, data, idx),\n",
        "            inputs=[radios[i], quiz_data_state],\n",
        "            outputs=results[i]\n",
        "        )\n",
        "\n",
        "    def score_all(*args):\n",
        "        *answers, quiz_data = args\n",
        "        if not quiz_data:\n",
        "            return \"‚ÑπÔ∏è Generate a quiz first.\"\n",
        "        total = len(quiz_data)\n",
        "        correct = 0\n",
        "        lines = []\n",
        "        for i in range(total):\n",
        "            sel = answers[i] if i < len(answers) else None\n",
        "            ans = quiz_data[i][\"answer\"]\n",
        "            if sel == ans:\n",
        "                correct += 1\n",
        "                lines.append(f\"**Q{i+1}.** ‚úÖ Correct\")\n",
        "            elif sel is None:\n",
        "                lines.append(f\"**Q{i+1}.** ‚è≠Ô∏è Skipped (Correct: **{ans}**)\")\n",
        "            else:\n",
        "                lines.append(f\"**Q{i+1}.** ‚ùå Wrong (Your: _{sel}_ | Correct: **{ans}**)\")\n",
        "        lines.append(f\"\\n### üéØ Final Score: **{correct}/{total}**\")\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    submit_btn.click(\n",
        "        score_all,\n",
        "        inputs=radios + [quiz_data_state],\n",
        "        outputs=score_md\n",
        "    )\n",
        "    if __name__ == \"__main__\":\n",
        "        demo.launch(server_name=\"0.0.0.0\")\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "# üåü AI-Powered MCQ Generator\n",
        "Upload a PDF or Paste Text ‚Üí Get **Instant Quiz Questions**!\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "DVXmBLGNIs6j",
        "outputId": "810331a2-da03-45d1-9ec0-2c9b0dbf1048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f66ac5358c5f496322.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f66ac5358c5f496322.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Markdown(\"\"\"\n",
        "# üåü AI-Powered MCQ Generator\n",
        "Upload a PDF or Paste Text ‚Üí Get **Instant Quiz Questions**!\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95NMfyCZEe6S",
        "outputId": "ce43eef0-987d-4527-b57b-37979c0121e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gradio.components.markdown.Markdown at 0x7b45a140cc50>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}